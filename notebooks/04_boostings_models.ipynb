{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21fe9789",
   "metadata": {},
   "source": [
    "# Boosting Models\n",
    "\n",
    "**Date:** 11/11/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4664df2e",
   "metadata": {},
   "source": [
    "* https://xgboost.readthedocs.io/en/stable/tutorials/model.html\n",
    "* https://lightgbm.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee2fe1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /Users/cesar/sandbox/ai_programming_foundations/venv/lib/python3.12/site-packages (3.1.1)\n",
      "Requirement already satisfied: lightgbm in /Users/cesar/sandbox/ai_programming_foundations/venv/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy in /Users/cesar/sandbox/ai_programming_foundations/venv/lib/python3.12/site-packages (from xgboost) (2.3.4)\n",
      "Requirement already satisfied: scipy in /Users/cesar/sandbox/ai_programming_foundations/venv/lib/python3.12/site-packages (from xgboost) (1.16.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7165fb5e-30b8-4668-88e7-62db15162b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mJupyter detected\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[1;32m2\u001b[0m\u001b[1;32m channel Terms of Service accepted\u001b[0m\n",
      "doneieving notices: - \n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: osx-64\n",
      "doneecting package metadata (repodata.json): / \n",
      "doneing environment: | \n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.5.1\n",
      "    latest version: 25.7.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - py-xgboost\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _py-xgboost-mutex-2.0      |            cpu_0           8 KB  conda-forge\n",
      "    ca-certificates-2025.11.12 |       hbd8a1cb_0         149 KB  conda-forge\n",
      "    certifi-2025.11.12         |     pyhd8ed1ab_0         153 KB  conda-forge\n",
      "    conda-25.7.0               |  py313habf4b1d_0         1.2 MB  conda-forge\n",
      "    libxgboost-2.1.1           |       hab2016f_0         1.9 MB\n",
      "    openssl-3.6.0              |       h230baf5_0         2.7 MB  conda-forge\n",
      "    py-xgboost-2.1.1           | cpu_pyh684852a_0         131 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         6.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _py-xgboost-mutex  conda-forge/osx-64::_py-xgboost-mutex-2.0-cpu_0 \n",
      "  libxgboost         pkgs/main/osx-64::libxgboost-2.1.1-hab2016f_0 \n",
      "  py-xgboost         conda-forge/noarch::py-xgboost-2.1.1-cpu_pyh684852a_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main/osx-64::ca-certificates-202~ --> conda-forge/noarch::ca-certificates-2025.11.12-hbd8a1cb_0 \n",
      "  certifi            pkgs/main/osx-64::certifi-2025.4.26-p~ --> conda-forge/noarch::certifi-2025.11.12-pyhd8ed1ab_0 \n",
      "  conda              pkgs/main::conda-25.5.1-py313hecd8cb5~ --> conda-forge::conda-25.7.0-py313habf4b1d_0 \n",
      "  openssl              pkgs/main::openssl-3.0.16-h184c1cd_0 --> conda-forge::openssl-3.6.0-h230baf5_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "openssl-3.6.0        | 2.7 MB    |                                       |   0% \n",
      "libxgboost-2.1.1     | 1.9 MB    |                                       |   0% \u001b[A\n",
      "\n",
      "conda-25.7.0         | 1.2 MB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "certifi-2025.11.12   | 153 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2025 | 149 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "py-xgboost-2.1.1     | 131 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_py-xgboost-mutex-2. | 8 KB      |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "conda-25.7.0         | 1.2 MB    | 4                                     |   1% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.6.0        | 2.7 MB    | 2                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2025 | 149 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2025 | 149 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "certifi-2025.11.12   | 153 KB    | ###8                                  |  10% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "openssl-3.6.0        | 2.7 MB    | ###########3                          |  31% \u001b[A\u001b[A\n",
      "\n",
      "conda-25.7.0         | 1.2 MB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "certifi-2025.11.12   | 153 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "py-xgboost-2.1.1     | 131 KB    | ####5                                 |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "libxgboost-2.1.1     | 1.9 MB    | 2                                     |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "py-xgboost-2.1.1     | 131 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "certifi-2025.11.12   | 153 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "openssl-3.6.0        | 2.7 MB    | ###########################           |  73% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.6.0        | 2.7 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "libxgboost-2.1.1     | 1.9 MB    | ##############9                       |  40% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_py-xgboost-mutex-2. | 8 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_py-xgboost-mutex-2. | 8 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "libxgboost-2.1.1     | 1.9 MB    | ##################################### | 100% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_py-xgboost-mutex-2. | 8 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "libxgboost-2.1.1     | 1.9 MB    | ##################################### | 100% \u001b[A\n",
      "openssl-3.6.0        | 2.7 MB    | ##################################### | 100% \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "donearing transaction: | \n",
      "donefying transaction: - \n",
      "doneuting transaction: - \n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge py-xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03972c5c",
   "metadata": {},
   "source": [
    "# 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "996644ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4a001",
   "metadata": {},
   "source": [
    "## 1.0 # Boosting, XGBoost y LightGBM para Modelos de Regresión\n",
    "\n",
    "## 1.0.1 Boosting para Regresión\n",
    "\n",
    "### Idea general\n",
    "\n",
    "**Boosting** es una técnica de *ensemblado secuencial* que combina **muchos modelos débiles** (por lo general, pequeños árboles de decisión llamados *stumps*) para formar un **modelo fuerte y preciso**.\n",
    "\n",
    "A diferencia de los **Random Forests**, donde los árboles se entrenan **en paralelo e independientemente**, en el **Boosting** los árboles se entrenan **uno después de otro**, y cada nuevo árbol **corrige los errores del anterior**.\n",
    "\n",
    "---\n",
    "\n",
    "### Intuición paso a paso\n",
    "\n",
    "1. Se comienza con una predicción inicial simple, por ejemplo:\n",
    "   $$\n",
    "   \\hat{y}^{(0)} = \\bar{y} = \\frac{1}{N}\\sum_{i=1}^{N} y_i\n",
    "   $$\n",
    "   (el promedio de los valores reales).\n",
    "\n",
    "2. En cada iteración $ m = 1, 2, \\dots, M $:\n",
    "   - Se calculan los **residuos** (errores actuales):\n",
    "     $$\n",
    "     r_i^{(m)} = y_i - \\hat{y}_i^{(m-1)}\n",
    "     $$\n",
    "   - Se entrena un **nuevo árbol de regresión** para predecir estos residuos.\n",
    "   - Se actualiza la predicción del modelo:\n",
    "     $$\n",
    "     \\hat{y}_i^{(m)} = \\hat{y}_i^{(m-1)} + \\eta \\, h_m(x_i)\n",
    "     $$\n",
    "     donde:\n",
    "     - $ h_m(x_i) $ es la predicción del árbol $ m $,\n",
    "     - $ \\eta $ (learning rate) controla cuánto peso se da a cada nuevo árbol.\n",
    "\n",
    "3. El modelo final es la suma ponderada de los árboles:\n",
    "   $$\n",
    "   \\hat{f}(x) = \\sum_{m=1}^{M} \\eta \\, h_m(x)\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### Características clave del Boosting\n",
    "\n",
    "- Cada árbol intenta **corregir los errores residuales** del modelo anterior.  \n",
    "- Se da **más peso a las observaciones mal predichas**.  \n",
    "- El parámetro **learning rate ($ \\eta $)** controla la velocidad de aprendizaje:\n",
    "  - Valores pequeños (p. ej. 0.05) → aprendizaje más lento pero más estable.  \n",
    "  - Valores grandes → riesgo de sobreajuste.\n",
    "\n",
    "---\n",
    "\n",
    "### Ventajas y desventajas\n",
    "\n",
    "| Ventajas | Desventajas |\n",
    "|-----------|--------------|\n",
    "| Alta precisión | Más lento de entrenar |\n",
    "| Menor sesgo que Random Forest | Sensible al ruido |\n",
    "| Captura relaciones complejas | Muchos hiperparámetros |\n",
    "\n",
    "---\n",
    "\n",
    "## 2 XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "### Qué es\n",
    "\n",
    "**XGBoost** es una implementación optimizada del algoritmo de Boosting basada en el concepto de **Gradient Boosting**.  \n",
    "Fue diseñada para ser **rápida, escalable y regularizada**, lo que la convierte en una de las herramientas más populares en *machine learning*.\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient Boosting: concepto matemático\n",
    "\n",
    "En lugar de ajustar los residuos directamente, XGBoost ajusta **la dirección del gradiente del error** respecto a las predicciones actuales.\n",
    "\n",
    "Dado un conjunto de datos $(x_i, y_i)$ y una función de pérdida $ L(y_i, \\hat{y}_i) $, el objetivo es minimizar:\n",
    "\n",
    "$$\n",
    "\\text{Obj} = \\sum_{i=1}^{N} L(y_i, \\hat{y}_i) + \\sum_{m=1}^{M} \\Omega(f_m)\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $ L $ mide el error de predicción (por ejemplo, MSE),\n",
    "- $ \\Omega(f_m) $ penaliza la complejidad de los árboles, para evitar sobreajuste.\n",
    "\n",
    "---\n",
    "\n",
    "### Actualización por gradiente\n",
    "\n",
    "En cada iteración $ m $, XGBoost construye un árbol $ f_m(x) $ que aproxima el **gradiente negativo** de la función de pérdida:\n",
    "\n",
    "$$\n",
    "g_i^{(m)} = \\frac{\\partial L(y_i, \\hat{y}_i^{(m-1)})}{\\partial \\hat{y}_i^{(m-1)}}\n",
    "$$\n",
    "\n",
    "Luego ajusta el árbol para minimizar una **aproximación de segundo orden** de la función de pérdida:\n",
    "\n",
    "$$\n",
    "\\text{Obj}^{(m)} \\approx \\sum_{i=1}^{N} \\left[ g_i^{(m)} f_m(x_i) + \\frac{1}{2} h_i^{(m)} f_m^2(x_i) \\right] + \\Omega(f_m)\n",
    "$$\n",
    "\n",
    "donde $ h_i^{(m)} $ es la segunda derivada (Hessiano).\n",
    "\n",
    "---\n",
    "\n",
    "### Regularización\n",
    "\n",
    "XGBoost incluye una función de penalización para controlar la complejidad de los árboles:\n",
    "\n",
    "$$\n",
    "\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $ T $ es el número de hojas,\n",
    "- $ w_j $ son los pesos de cada hoja,\n",
    "- $ \\gamma $ y $ \\lambda $ son parámetros de regularización.\n",
    "\n",
    "Esto reduce el sobreajuste y mejora la generalización.\n",
    "\n",
    "---\n",
    "\n",
    "### Ventajas de XGBoost\n",
    "\n",
    "* Muy rápido (usa paralelización y memoria eficiente).  \n",
    "* Regularización integrada ($ L_1 $ y $ L_2 $). \n",
    "* Maneja datos faltantes de forma nativa.\n",
    "*  Permite árboles profundos y controlados.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3 LightGBM (Light Gradient Boosting Machine)\n",
    "\n",
    "### Qué es\n",
    "\n",
    "**LightGBM** es otra implementación de *gradient boosting*, desarrollada por Microsoft.  \n",
    "Se diseñó para ser **aún más rápida y eficiente** que XGBoost, especialmente en conjuntos de datos grandes y de alta dimensión.\n",
    "\n",
    "---\n",
    "\n",
    "### Diferencia clave: método de crecimiento del árbol\n",
    "\n",
    "Mientras que XGBoost crece los árboles **nivel por nivel (level-wise)**,  \n",
    "**LightGBM crece los árboles hoja por hoja (leaf-wise)**, eligiendo siempre la hoja con la mayor ganancia de información.\n",
    "\n",
    "* **XGBoost (level-wise):**\n",
    "- Expande todos los nodos del mismo nivel.\n",
    "- Más equilibrado, menor sobreajuste.\n",
    "\n",
    "* **LightGBM (leaf-wise):**\n",
    "- Expande la hoja con mayor ganancia.\n",
    "- Puede generar árboles más profundos y precisos.\n",
    "- Mayor riesgo de sobreajuste (controlado con `max_depth`).\n",
    "\n",
    "---\n",
    "\n",
    "### Optimizaciones adicionales\n",
    "\n",
    "1. **Histogram-based learning:**  \n",
    "   Convierte los valores continuos en *bins* (intervalos), reduciendo el costo computacional.\n",
    "\n",
    "2. **Gradient-based One-Side Sampling (GOSS):**  \n",
    "   Usa solo una fracción de las muestras con grandes gradientes (más informativas).\n",
    "\n",
    "3. **Exclusive Feature Bundling (EFB):**  \n",
    "   Combina características mutuamente excluyentes en una sola, reduciendo la dimensionalidad.\n",
    "\n",
    "---\n",
    "\n",
    "### Fórmula de actualización (similar a XGBoost)\n",
    "\n",
    "$$\n",
    "\\hat{y}_i^{(m)} = \\hat{y}_i^{(m-1)} + \\eta \\, f_m(x_i)\n",
    "$$\n",
    "\n",
    "pero con un método de partición más eficiente y un cálculo de histograma optimizado.\n",
    "\n",
    "---\n",
    "\n",
    "### Ventajas de LightGBM\n",
    "\n",
    "* Entrenamiento extremadamente rápido.  \n",
    "* Escala muy bien con millones de filas y columnas.  \n",
    "* Alta precisión con datos grandes.  \n",
    "* Bajo consumo de memoria.\n",
    "\n",
    "---\n",
    "\n",
    "### Desventajas\n",
    "\n",
    "* Puede sobreajustar si los datos son pequeños.  \n",
    "* Menos interpretable.  \n",
    "* Parámetros más sensibles que en XGBoost.\n",
    "\n",
    "---\n",
    "\n",
    "## Resumen\n",
    "\n",
    "| Característica | Boosting Clásico | XGBoost | LightGBM |\n",
    "|----------------|------------------|----------|-----------|\n",
    "| Tipo de boosting | Residual | Gradient (2° orden) | Gradient (2° orden optimizado) |\n",
    "| Regularización | No explícita | Sí (L1 y L2) | Sí |\n",
    "| Crecimiento del árbol | Secuencial simple | Nivel por nivel | Hoja por hoja |\n",
    "| Velocidad | Media | Rápido | Muy rápido |\n",
    "| Escalabilidad | Media | Alta | Muy alta |\n",
    "| Riesgo de overfitting | Medio | Bajo | Medio-Alto |\n",
    "| Ideal para | Datasets medianos | Datos estructurados | Big data/tablas muy grandes |\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusión\n",
    "\n",
    "- **Boosting:** combina modelos débiles secuencialmente para mejorar el rendimiento.  \n",
    "- **XGBoost:** implementación optimizada del gradient boosting con regularización y segunda derivada.  \n",
    "- **LightGBM:** versión aún más rápida y escalable, ideal para grandes volúmenes de datos.\n",
    "\n",
    "Ambos (XGBoost y LightGBM) se han convertido en **estándares industriales** por su eficiencia, precisión y capacidad de generalización en problemas de regresión y clasificación.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006fec1e",
   "metadata": {},
   "source": [
    "## 1.1. XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9974ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos datos de viviendas en California\n",
    "datos = fetch_california_housing()\n",
    "X, y = datos.data, datos.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "986776e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd5d8a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Train (RMSE): 0.3689490478304685\n",
      "Error Test (RMSE): 0.47672034565362764\n"
     ]
    }
   ],
   "source": [
    "# Convertimos a formato DMatrix (optimizado para XGBoost)\n",
    "xgb_dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "xgb_dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Configuración del modelo\n",
    "xgb_parameters = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.1  # Tasa de aprendizaje\n",
    "}\n",
    "\n",
    "# Entrenamiento\n",
    "modelo = xgb.train(xgb_parameters, xgb_dtrain, num_boost_round=100)\n",
    "\n",
    "# Predicción\n",
    "y_train_preds = modelo.predict(xgb_dtrain)\n",
    "print(\"Error Train (RMSE):\", root_mean_squared_error(y_train, y_train_preds))\n",
    "\n",
    "y_test_preds = modelo.predict(xgb_dtest)\n",
    "print(\"Error Test (RMSE):\", root_mean_squared_error(y_test, y_test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0960c563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5ef4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e13aa6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_dtrain = lgb.Dataset(X_train, y_train)\n",
    "lgb_dtest = lgb.Dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0936757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_parameters = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"num_leaves\": 31,\n",
    "    \"learning_rate\": 0.05\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b24b3213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.071947\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.train(lgb_parameters,lgb_dtrain, num_boost_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a2eaafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Train (RMSE): 0.4481941166091315\n",
      "Error Test (RMSE): 0.4882208769040563\n"
     ]
    }
   ],
   "source": [
    "# Predicción\n",
    "y_train_preds = lgb_model.predict(X_train)\n",
    "print(\"Error Train (RMSE):\", root_mean_squared_error(y_train, y_train_preds))\n",
    "\n",
    "y_test_preds = lgb_model.predict(X_test)\n",
    "print(\"Error Test (RMSE):\", root_mean_squared_error(y_test, y_test_preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
