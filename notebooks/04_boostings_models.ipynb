{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21fe9789",
   "metadata": {},
   "source": [
    "# Boosting Models\n",
    "\n",
    "**Date:** 11/11/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4664df2e",
   "metadata": {},
   "source": [
    "* https://xgboost.readthedocs.io/en/stable/tutorials/model.html\n",
    "* https://lightgbm.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee2fe1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install xgboost lightgbm catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03972c5c",
   "metadata": {},
   "source": [
    "# 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996644ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4a001",
   "metadata": {},
   "source": [
    "# 1.0 Boosting, XGBoost y LightGBM para Modelos de Regresión\n",
    "\n",
    "## 1.0.1 Boosting para Regresión\n",
    "\n",
    "### Idea general\n",
    "\n",
    "**Boosting** es una técnica de *ensemblado secuencial* que combina **muchos modelos débiles** (por lo general, pequeños árboles de decisión llamados *stumps*) para formar un **modelo fuerte y preciso**.\n",
    "\n",
    "A diferencia de los **Random Forests**, donde los árboles se entrenan **en paralelo e independientemente**, en el **Boosting** los árboles se entrenan **uno después de otro**, y cada nuevo árbol **corrige los errores del anterior**.\n",
    "\n",
    "---\n",
    "\n",
    "### Intuición paso a paso\n",
    "\n",
    "1. Se comienza con una predicción inicial simple, por ejemplo:\n",
    "   $$\n",
    "   \\hat{y}^{(0)} = \\bar{y} = \\frac{1}{N}\\sum_{i=1}^{N} y_i\n",
    "   $$\n",
    "   (el promedio de los valores reales).\n",
    "\n",
    "2. En cada iteración $ m = 1, 2, \\dots, M $:\n",
    "   - Se calculan los **residuos** (errores actuales):\n",
    "     $$\n",
    "     r_i^{(m)} = y_i - \\hat{y}_i^{(m-1)}\n",
    "     $$\n",
    "   - Se entrena un **nuevo árbol de regresión** para predecir estos residuos.\n",
    "   - Se actualiza la predicción del modelo:\n",
    "     $$\n",
    "     \\hat{y}_i^{(m)} = \\hat{y}_i^{(m-1)} + \\eta \\, h_m(x_i)\n",
    "     $$\n",
    "     donde:\n",
    "     - $ h_m(x_i) $ es la predicción del árbol $ m $,\n",
    "     - $ \\eta $ (learning rate) controla cuánto peso se da a cada nuevo árbol.\n",
    "\n",
    "3. El modelo final es la suma ponderada de los árboles:\n",
    "   $$\n",
    "   \\hat{f}(x) = \\sum_{m=1}^{M} \\eta \\, h_m(x)\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### Características clave del Boosting\n",
    "\n",
    "- Cada árbol intenta **corregir los errores residuales** del modelo anterior.  \n",
    "- Se da **más peso a las observaciones mal predichas**.  \n",
    "- El parámetro **learning rate ($ \\eta $)** controla la velocidad de aprendizaje:\n",
    "  - Valores pequeños (p. ej. 0.05) → aprendizaje más lento pero más estable.  \n",
    "  - Valores grandes → riesgo de sobreajuste.\n",
    "\n",
    "---\n",
    "\n",
    "### Ventajas y desventajas\n",
    "\n",
    "| Ventajas | Desventajas |\n",
    "|-----------|--------------|\n",
    "| Alta precisión | Más lento de entrenar |\n",
    "| Menor sesgo que Random Forest | Sensible al ruido |\n",
    "| Captura relaciones complejas | Muchos hiperparámetros |\n",
    "\n",
    "---\n",
    "\n",
    "## 2 XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "### Qué es\n",
    "\n",
    "**XGBoost** es una implementación optimizada del algoritmo de Boosting basada en el concepto de **Gradient Boosting**.  \n",
    "Fue diseñada para ser **rápida, escalable y regularizada**, lo que la convierte en una de las herramientas más populares en *machine learning*.\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient Boosting: concepto matemático\n",
    "\n",
    "En lugar de ajustar los residuos directamente, XGBoost ajusta **la dirección del gradiente del error** respecto a las predicciones actuales.\n",
    "\n",
    "Dado un conjunto de datos $(x_i, y_i)$ y una función de pérdida $ L(y_i, \\hat{y}_i) $, el objetivo es minimizar:\n",
    "\n",
    "$$\n",
    "\\text{Obj} = \\sum_{i=1}^{N} L(y_i, \\hat{y}_i) + \\sum_{m=1}^{M} \\Omega(f_m)\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $ L $ mide el error de predicción (por ejemplo, MSE),\n",
    "- $ \\Omega(f_m) $ penaliza la complejidad de los árboles, para evitar sobreajuste.\n",
    "\n",
    "---\n",
    "\n",
    "### Actualización por gradiente\n",
    "\n",
    "En cada iteración $ m $, XGBoost construye un árbol $ f_m(x) $ que aproxima el **gradiente negativo** de la función de pérdida:\n",
    "\n",
    "$$\n",
    "g_i^{(m)} = \\frac{\\partial L(y_i, \\hat{y}_i^{(m-1)})}{\\partial \\hat{y}_i^{(m-1)}}\n",
    "$$\n",
    "\n",
    "Luego ajusta el árbol para minimizar una **aproximación de segundo orden** de la función de pérdida:\n",
    "\n",
    "$$\n",
    "\\text{Obj}^{(m)} \\approx \\sum_{i=1}^{N} \\left[ g_i^{(m)} f_m(x_i) + \\frac{1}{2} h_i^{(m)} f_m^2(x_i) \\right] + \\Omega(f_m)\n",
    "$$\n",
    "\n",
    "donde $ h_i^{(m)} $ es la segunda derivada (Hessiano).\n",
    "\n",
    "---\n",
    "\n",
    "### Regularización\n",
    "\n",
    "XGBoost incluye una función de penalización para controlar la complejidad de los árboles:\n",
    "\n",
    "$$\n",
    "\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $ T $ es el número de hojas,\n",
    "- $ w_j $ son los pesos de cada hoja,\n",
    "- $ \\gamma $ y $ \\lambda $ son parámetros de regularización.\n",
    "\n",
    "Esto reduce el sobreajuste y mejora la generalización.\n",
    "\n",
    "---\n",
    "\n",
    "### Ventajas de XGBoost\n",
    "\n",
    "* Muy rápido (usa paralelización y memoria eficiente).  \n",
    "* Regularización integrada ($ L_1 $ y $ L_2 $). \n",
    "* Maneja datos faltantes de forma nativa.\n",
    "*  Permite árboles profundos y controlados.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3 LightGBM (Light Gradient Boosting Machine)\n",
    "\n",
    "### Qué es\n",
    "\n",
    "**LightGBM** es otra implementación de *gradient boosting*, desarrollada por Microsoft.  \n",
    "Se diseñó para ser **aún más rápida y eficiente** que XGBoost, especialmente en conjuntos de datos grandes y de alta dimensión.\n",
    "\n",
    "---\n",
    "\n",
    "### Diferencia clave: método de crecimiento del árbol\n",
    "\n",
    "Mientras que XGBoost crece los árboles **nivel por nivel (level-wise)**,  \n",
    "**LightGBM crece los árboles hoja por hoja (leaf-wise)**, eligiendo siempre la hoja con la mayor ganancia de información.\n",
    "\n",
    "* **XGBoost (level-wise):**\n",
    "- Expande todos los nodos del mismo nivel.\n",
    "- Más equilibrado, menor sobreajuste.\n",
    "\n",
    "* **LightGBM (leaf-wise):**\n",
    "- Expande la hoja con mayor ganancia.\n",
    "- Puede generar árboles más profundos y precisos.\n",
    "- Mayor riesgo de sobreajuste (controlado con `max_depth`).\n",
    "\n",
    "---\n",
    "\n",
    "### Optimizaciones adicionales\n",
    "\n",
    "1. **Histogram-based learning:**  \n",
    "   Convierte los valores continuos en *bins* (intervalos), reduciendo el costo computacional.\n",
    "\n",
    "2. **Gradient-based One-Side Sampling (GOSS):**  \n",
    "   Usa solo una fracción de las muestras con grandes gradientes (más informativas).\n",
    "\n",
    "3. **Exclusive Feature Bundling (EFB):**  \n",
    "   Combina características mutuamente excluyentes en una sola, reduciendo la dimensionalidad.\n",
    "\n",
    "---\n",
    "\n",
    "### Fórmula de actualización (similar a XGBoost)\n",
    "\n",
    "$$\n",
    "\\hat{y}_i^{(m)} = \\hat{y}_i^{(m-1)} + \\eta \\, f_m(x_i)\n",
    "$$\n",
    "\n",
    "pero con un método de partición más eficiente y un cálculo de histograma optimizado.\n",
    "\n",
    "---\n",
    "\n",
    "### Ventajas de LightGBM\n",
    "\n",
    "* Entrenamiento extremadamente rápido.  \n",
    "* Escala muy bien con millones de filas y columnas.  \n",
    "* Alta precisión con datos grandes.  \n",
    "* Bajo consumo de memoria.\n",
    "\n",
    "---\n",
    "\n",
    "### Desventajas\n",
    "\n",
    "* Puede sobreajustar si los datos son pequeños.  \n",
    "* Menos interpretable.  \n",
    "* Parámetros más sensibles que en XGBoost.\n",
    "\n",
    "---\n",
    "\n",
    "## Resumen\n",
    "\n",
    "| Característica | Boosting Clásico | XGBoost | LightGBM |\n",
    "|----------------|------------------|----------|-----------|\n",
    "| Tipo de boosting | Residual | Gradient (2° orden) | Gradient (2° orden optimizado) |\n",
    "| Regularización | No explícita | Sí (L1 y L2) | Sí |\n",
    "| Crecimiento del árbol | Secuencial simple | Nivel por nivel | Hoja por hoja |\n",
    "| Velocidad | Media | Rápido | Muy rápido |\n",
    "| Escalabilidad | Media | Alta | Muy alta |\n",
    "| Riesgo de overfitting | Medio | Bajo | Medio-Alto |\n",
    "| Ideal para | Datasets medianos | Datos estructurados | Big data/tablas muy grandes |\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusión\n",
    "\n",
    "- **Boosting:** combina modelos débiles secuencialmente para mejorar el rendimiento.  \n",
    "- **XGBoost:** implementación optimizada del gradient boosting con regularización y segunda derivada.  \n",
    "- **LightGBM:** versión aún más rápida y escalable, ideal para grandes volúmenes de datos.\n",
    "\n",
    "Ambos (XGBoost y LightGBM) se han convertido en **estándares industriales** por su eficiencia, precisión y capacidad de generalización en problemas de regresión y clasificación.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006fec1e",
   "metadata": {},
   "source": [
    "## 1.1. XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9974ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos datos de viviendas en California\n",
    "datos = fetch_california_housing()\n",
    "X, y = datos.data, datos.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "986776e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd5d8a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Train (RMSE): 0.3689490478304685\n",
      "Error Test (RMSE): 0.47672034565362764\n"
     ]
    }
   ],
   "source": [
    "# Convertimos a formato DMatrix (optimizado para XGBoost)\n",
    "xgb_dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "xgb_dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Configuración del modelo\n",
    "xgb_parameters = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.1  # Tasa de aprendizaje\n",
    "}\n",
    "\n",
    "# Entrenamiento\n",
    "modelo = xgb.train(xgb_parameters, xgb_dtrain, num_boost_round=100)\n",
    "\n",
    "# Predicción\n",
    "y_train_preds = modelo.predict(xgb_dtrain)\n",
    "print(\"Error Train (RMSE):\", root_mean_squared_error(y_train, y_train_preds))\n",
    "\n",
    "y_test_preds = modelo.predict(xgb_dtest)\n",
    "print(\"Error Test (RMSE):\", root_mean_squared_error(y_test, y_test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0960c563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5ef4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e13aa6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_dtrain = lgb.Dataset(X_train, y_train)\n",
    "lgb_dtest = lgb.Dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0936757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_parameters = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"num_leaves\": 31,\n",
    "    \"learning_rate\": 0.05\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b24b3213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000918 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.071947\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.train(lgb_parameters,lgb_dtrain, num_boost_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a2eaafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Train (RMSE): 0.4481941166091315\n",
      "Error Test (RMSE): 0.4882208769040563\n"
     ]
    }
   ],
   "source": [
    "# Predicción\n",
    "y_train_preds = lgb_model.predict(X_train)\n",
    "print(\"Error Train (RMSE):\", root_mean_squared_error(y_train, y_train_preds))\n",
    "\n",
    "y_test_preds = lgb_model.predict(X_test)\n",
    "print(\"Error Test (RMSE):\", root_mean_squared_error(y_test, y_test_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c847e34",
   "metadata": {},
   "source": [
    "## 3. CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca4bd96",
   "metadata": {},
   "source": [
    "https://catboost.ai/docs/en/concepts/python-reference_catboostregressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7a4cd558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0897903\ttotal: 19ms\tremaining: 5.69s\n",
      "100:\tlearn: 0.4530808\ttotal: 529ms\tremaining: 1.04s\n",
      "200:\tlearn: 0.3907115\ttotal: 1.16s\tremaining: 573ms\n",
      "299:\tlearn: 0.3532218\ttotal: 1.65s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x11fff74a0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Initialize the CatBoostRegressor with RMSE as the loss function\n",
    "cb_model = CatBoostRegressor(\n",
    "    loss_function='RMSE',\n",
    "    max_depth=8,\n",
    "    learning_rate=0.1,\n",
    "    #n_estimators=300,\n",
    "    num_boost_round=300,\n",
    "    min_data_in_leaf=10,\n",
    "    )\n",
    "\n",
    "# Fit the model on the training data with verbose logging every 100 iterations\n",
    "cb_model.fit(X_train, y_train, verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "51870044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Train (RMSE): 0.35322184789597055\n",
      "Error Test (RMSE): 0.4525225693589356\n"
     ]
    }
   ],
   "source": [
    "# Predicción\n",
    "y_train_preds = cb_model.predict(X_train)\n",
    "print(\"Error Train (RMSE):\", root_mean_squared_error(y_train, y_train_preds))\n",
    "\n",
    "y_test_preds = cb_model.predict(X_test)\n",
    "print(\"Error Test (RMSE):\", root_mean_squared_error(y_test, y_test_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
